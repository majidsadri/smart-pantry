{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"data\"\n",
    "\n",
    "data = {\n",
    "    \"sample\": [],\n",
    "    \"finetuned\": [],\n",
    "    \"vanilla\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "for dataset in data.keys():\n",
    "    with open(os.path.join(data_dir, \"{}.pickle\".format(dataset)), 'rb') as fw:\n",
    "        data[dataset] = pickle.load(fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['finetuned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6405174905590969"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim([data[\"sample\"][0], data[\"finetuned\"][0]])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg:  0.6646976850693705\n"
     ]
    }
   ],
   "source": [
    "from cosine_similarity import cosine_sim\n",
    "\n",
    "avg = 0\n",
    "for k, rec1 in enumerate(data[\"sample\"]):\n",
    "    best = 0\n",
    "    for i in range(0,10):\n",
    "        rec2 = data[\"finetuned\"][k*10 + i]\n",
    "        cos = cosine_sim([rec1, rec2])[0,1]\n",
    "        best = max(best, cos)\n",
    "    avg += best\n",
    "\n",
    "avg = avg/len(data[\"sample\"])\n",
    "print(\"avg: \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match({'fromy': 17, 'fromx': 13, 'toy': 17, 'tox': 24, 'ruleId': 'MORFOLOGIK_RULE_EN_US', 'msg': 'Possible spelling mistake found', 'replacements': [], 'context': '...almond extract (optional)  1 cup 2 cups all-purpose flour  5 tablespoons 2 teaspoons cocoa ...', 'contextoffset': 43, 'offset': 409, 'errorlength': 11, 'category': 'Possible Typo', 'locqualityissuetype': 'misspelling'})]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "#tool.disable_spellchecking()\n",
    "results = tool.check(data[\"finetuned\"][0])\n",
    "results_filtered = [result for result in results if result.ruleId!='WHITESPACE_RULE' ]\n",
    "results_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.388\n"
     ]
    }
   ],
   "source": [
    "import language_check\n",
    "tool = language_check.LanguageTool('en-US')\n",
    "#tool.disable_spellchecking()\n",
    "\n",
    "avg = 0\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    results = tool.check(rec)\n",
    "    results_filtered = [result for result in results if result.ruleId!='WHITESPACE_RULE' ]\n",
    "    avg += len(results_filtered)\n",
    "\n",
    "print(avg/len(data[dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readibility\n",
    "* textstat.smog_index(test_data)\n",
    "* textstat.flesch_kincaid_grade(test_data)\n",
    "* textstat.coleman_liau_index(test_data)\n",
    "* textstat.automated_readability_index(test_data)\n",
    "* textstat.dale_chall_readability_score(test_data)\n",
    "* textstat.difficult_words(test_data)\n",
    "* textstat.linsear_write_formula(test_data)\n",
    "* textstat.gunning_fog(test_data)\n",
    "* textstat.text_standard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.74174 38.185 ModeResult(mode=array([-46.27]), count=array([4]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import textstat\n",
    "from scipy import stats\n",
    "\n",
    "ret = []\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.flesch_reading_ease(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.015 0.0 ModeResult(mode=array([0.]), count=array([54]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import textstat\n",
    "from scipy import stats\n",
    "\n",
    "ret = []\n",
    "dataset = \"sample\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.smog_index(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.68294 20.59 ModeResult(mode=array([10.96]), count=array([4]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import textstat\n",
    "from scipy import stats\n",
    "\n",
    "ret = []\n",
    "dataset = \"finetuned\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.gunning_fog(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.419099999999998 8.58 ModeResult(mode=array([7.05]), count=array([3]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import textstat\n",
    "from scipy import stats\n",
    "\n",
    "ret = []\n",
    "dataset = \"sample\"\n",
    "\n",
    "for rec in data[dataset]:\n",
    "    result = textstat.dale_chall_readability_score(rec)\n",
    "    #print(result)\n",
    "    ret.append(result)\n",
    "    \n",
    "print(np.mean(ret), np.median(ret), stats.mode(ret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import nltk.translate.gleu_score as gleu\n",
    "import nltk.translate.meteor_score as meteor\n",
    "\n",
    "def wer_count(hyp, ref, print_matrix=False):\n",
    "    N = len(hyp)\n",
    "    M = len(ref)\n",
    "    L = np.zeros((N,M))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            if min(i,j) == 0:\n",
    "                L[i,j] = max(i,j)\n",
    "            else:\n",
    "                deletion = L[i-1,j] + 1\n",
    "                insertion = L[i,j-1] + 1\n",
    "                sub = 1 if hyp[i] != ref[j] else 0\n",
    "                substitution = L[i-1,j-1] + sub\n",
    "                L[i,j] = min(deletion, min(insertion, substitution))\n",
    "    return int(L[N-1, M-1])\n",
    "\n",
    "def bleu_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score_ref_a = bleu.sentence_bleu(refs, hyp, smoothing_function=smoothie)\n",
    "    return score_ref_a\n",
    "\n",
    "def gleu_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "    score_ref_a = gleu.sentence_gleu(refs, hyp)\n",
    "    return score_ref_a\n",
    "\n",
    "from jiwer import wer, mer\n",
    "\n",
    "def wer_score(recipe, refer):\n",
    "    hyp = recipe\n",
    "    refs = refer\n",
    "\n",
    "    mn = 99999\n",
    "    for ref in refs:\n",
    "        b = wer(ref, hyp)\n",
    "        mn = min(mn, b)\n",
    "       \n",
    "    return mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 62.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8457450731782216"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"finetuned\"][k*10: k*10 + 10]\n",
    "    res = bleu_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 39.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5065526994991351"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"vanilla\"][k*10: k*10 + 10]\n",
    "    res = gleu_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 214.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8810920377557072"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ret = []\n",
    "for k, rec1 in enumerate(tqdm(data[\"sample\"])):\n",
    "    rec2 = data[\"vanilla\"][k*10: k*10 + 10]\n",
    "    res = wer_score(rec1, rec2)\n",
    "    ret.append(res)\n",
    "\n",
    "np.mean(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
