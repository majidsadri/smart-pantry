{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allrecipes\n",
    "Set of some tasks related to scraping allrecipes.com  \n",
    "It does not aim to be pretty, it aims to be usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "with open('links.txt') as f:\n",
    "    links = list(map(lambda x: x.strip(), f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrecipes get directions\n",
    "def directions_allrecipes(soup):\n",
    "    temp = list(map(lambda x: x.get_text().strip(), soup.find_all('span', 'recipe-directions__list--item')))\n",
    "    temp = list(filter(lambda x: x != '', temp))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrecipes get ingredients\n",
    "def ingredients_allrecipes(soup):\n",
    "    temp = list(map(lambda x: x.get_text().strip(), soup.find_all('span', 'recipe-ingred_txt')))\n",
    "    temp = list(filter(lambda x: x != '' and x != 'Add all ingredients to list', temp))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrecipes get title\n",
    "def title_allrecipes(soup):\n",
    "    return list(map(lambda x: x.get_text().strip(), soup.find_all('h1','recipe-summary__h1')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the proper function to be used in future and in framework\n",
    "def save_recipe(step, url, retrieve_ingredients, retrieve_directions, retrieve_title, path=\"./\", filename_prefix=\"\", filename_suffix=\"\"):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        # get page\n",
    "        html_doc = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        # parse\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        # retrive information and save to dictionary\n",
    "        title = retrieve_title(soup)\n",
    "        recipe = dict()\n",
    "        recipe['title'] = title\n",
    "        recipe['ingredients'] = retrieve_ingredients(soup)\n",
    "        recipe['directions'] = retrieve_directions(soup)\n",
    "        # save to file\n",
    "        with open(path+'/'+filename_prefix+title.lower().replace(' ','_')+filename_suffix+'.json', 'w+') as f:\n",
    "            f.write(json.dumps(recipe))\n",
    "            \n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        \n",
    "        print(\"Step:\\t\", step, \"Time:\\t\", elapsed)\n",
    "    \n",
    "    except:\n",
    "        print(\"Unable to get recipe from:\\t\", url)\n",
    "        #for s in sys.exc_info():\n",
    "        #    print(s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#\n",
    "# only downloads links from the list\n",
    "#\n",
    "\n",
    "sleep_time = 1.1\n",
    "max_processes = 8\n",
    "counter = 0\n",
    "\n",
    "processes = []\n",
    "for i in range(1, 10):\n",
    "    # url selection or other custom shit here\n",
    "    str_id = '0' * (8 - len(str(i))) + str(i) # since names/titles are not unique, id is added\n",
    "    \n",
    "    arguments = (i, links[i], ingredients_allrecipes, directions_allrecipes, title_allrecipes, 'allrecipes/temp', str_id+'-')\n",
    "    \n",
    "    target_func = save_recipe\n",
    "    \n",
    "    ### try not to edit code below this line\n",
    "    \n",
    "    counter += 1\n",
    "    inactive = []\n",
    "    # visit list of processes\n",
    "    for proc in processes:\n",
    "        # when process is no longer active, join it and add to list of inactive processes\n",
    "        if not proc.is_alive():\n",
    "            proc.join()\n",
    "            inactive.append(proc)\n",
    "    # remove inactive processes from processes list\n",
    "    while inactive:\n",
    "        processes.remove(inactive.pop())\n",
    "    # print(\"Number of active processes:\\t\", len(processes))\n",
    "    \n",
    "    # if number of active processes is acceptable, we can start new process\n",
    "    if len(processes) < max_processes: \n",
    "        p = mp.Process(target = target_func, args = arguments)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    else:\n",
    "        print(\"List of processes is full\")\n",
    "    \n",
    "    # sleep, to avoid ddos attack or to fit in robots.txt rules\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# join remaining processes\n",
    "while processes:\n",
    "    temp = processes.pop()\n",
    "    temp.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing files\n",
    "Find files which were not downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawled.txt') as f:\n",
    "    links = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    \n",
    "ids = list(map(lambda x: int(x.split('-')[0]), links))\n",
    "\n",
    "with open('links.txt') as f:\n",
    "    alllinks = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    \n",
    "missing = []\n",
    "for i in range(len(alllinks)):\n",
    "    if i not in ids:\n",
    "        missing.append(alllinks[i])\n",
    "\n",
    "with open('missing.txt', 'w+') as f:\n",
    "    for m in missing:\n",
    "        f.write(m + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding links to recipes\n",
    "Since allrecipes.com was scraped in very early phase, the original files had not contained source urls, so to avoid processing this wevsite again, it was decided to add links from links list to recipes, matching by recipe title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'recipes_old'\n",
    "target_dir = 'recipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "with open('links.txt') as f:\n",
    "    links = list(map(lambda x: x.strip(), f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir(source_dir)\n",
    "filenames.remove('.ipynb_checkpoints')\n",
    "filenames.sort()\n",
    "linknames = list(map(lambda x: (x.split('/')[-2], x), links))\n",
    "linknames_dict = dict(linknames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching = []\n",
    "notmatching = 0\n",
    "for f in filenames:\n",
    "    with open(source_dir + '/' + f) as jf:\n",
    "        rec = json.load(jf)\n",
    "    name = rec['title'].lower().replace(' ', '-').replace('\\'', '').replace(',', '').replace('\"', '').replace('®', '').replace('™', '')\n",
    "    name = name.replace('!', '').replace('(','').replace(')','').replace(':', '').replace('.','').replace('&-', '').replace('---', '-').replace('--', '-')\n",
    "    if name in linknames_dict:\n",
    "        rec['link'] = linknames_dict[name]\n",
    "        matching.append(rec)\n",
    "    else:\n",
    "        #print(name)\n",
    "        notmatching += 1\n",
    "        #pass\n",
    "    \n",
    "notmatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62022"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(matching):\n",
    "    str_id = '0' * (8 - len(str(i))) + str(i)\n",
    "    with open(target_dir + '/' + str_id + '-' + m['title'].lower().replace(' ','_') + '.json', 'w+') as wf:\n",
    "        wf.write(json.dumps(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62022\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls recipes | wc -l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
