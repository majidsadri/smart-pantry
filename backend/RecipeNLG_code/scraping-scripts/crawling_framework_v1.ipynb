{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping framework - simple\n",
    "Retrives recipes from web pages, relies on provided set of urls. Do not allow for any kind of site structore discovery or so. Reliable and quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import urllib.request\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators\n",
    "These functions are responsible for returning list of links to be crawled and also an sequentional number. This function **must** take two arguments `begin` and `end` wich are basically python `range` arguments. Moreover this function **must** return two elements: *url* and *step* (between range borders). Here is how generator should look like:\n",
    "\n",
    "```python\n",
    "def sample_generator(begin, end):\n",
    "    for i in range(begin, end):\n",
    "        yield 'http://example.com', i\n",
    "```\n",
    "\n",
    "Samples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cookbooks_generator(begin, end):\n",
    "    seed = 'http://www.cookbooks.com/Recipe-Details.aspx?id='\n",
    "    for i in range(begin, end):\n",
    "        yield seed + str(i), i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allrecipes_generator(begin, end):\n",
    "    with open('allrecipes/links.txt') as f:\n",
    "        links = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    for i in range(begin, end):\n",
    "        yield links[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipesplus_generator(begin, end):\n",
    "    seed = 'http://recipes-plus.com/api/v2.0/recipes/'\n",
    "    with open('recipes-plus/ids.txt') as f:\n",
    "        ids = list(map(lambda x: x.strip(), f.readlines()))\n",
    "    for i in range(min(begin, len(ids)), min(end, len(ids))):\n",
    "        yield seed + ids[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use this assertions to check if your generator function does right\n",
    "test_generator = recipesplus_generator\n",
    "\n",
    "temp = list(test_generator(0,4))\n",
    "assert type(temp) is list\n",
    "assert len(temp) == 4\n",
    "assert type(temp[0]) is tuple\n",
    "assert len(temp[0]) == 2\n",
    "for t1, t2 in temp:\n",
    "    assert type(t1) is str\n",
    "    assert len(t1) > 0\n",
    "    # dummy url check\n",
    "    assert t1.find('http://') == 0 or t1.find('https://') == 0\n",
    "    assert t1.find('.') > 0\n",
    "    assert type(t2) is int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients\n",
    "This function should find all **ingredients** on the page. Takes single argument `soup` (BeautifulSoup), returns not empty *list* of ingredients (with quantities) as simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cookbooks get ingredients\n",
    "def ingredients_cookbooks(soup):\n",
    "    ingredients_tag = soup.find_all('span', 'H2', string='ingredients')[0]\n",
    "    return ingredients_tag.parent.p.get_text('|').strip().split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrecipes get ingredients\n",
    "def ingredients_allrecipes(soup):\n",
    "    temp = list(map(lambda x: x.get_text().strip(), soup.find_all('span', 'recipe-ingred_txt')))\n",
    "    temp = list(filter(lambda x: x != '' and x != 'Add all ingredients to list', temp))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipesplus get ingredients\n",
    "def ingredients_recipesplus(recipe):\n",
    "    func = lambda x: x[\"amount\"] + ' ' + x[\"unit\"] + ' ' + x['ingredient']\n",
    "    return list(map(func, recipe['ingredients']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 medium orange',\n",
       " '100 gram packet',\n",
       " '1 knuckle size strip of watermelon flesh',\n",
       " '1 smaller knuckle size feta cheese']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('recipes-plus/test.json') as f:\n",
    "    data = json.load(f)\n",
    "ingredients_recipesplus(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directions\n",
    "This function should find all **directions** on the page. Takes single argument `soup` (BeautifulSoup), returns not empty *list* of directions as simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cookbooks get directions\n",
    "def directions_cookbooks(soup):\n",
    "    directions_tag = soup.find_all('span', 'H2', string='preparation')[0]\n",
    "    # print(directions_tag)\n",
    "    return directions_tag.parent.p.get_text('|').strip().split('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrecipes get directions\n",
    "def directions_allrecipes(soup):\n",
    "    temp = list(map(lambda x: x.get_text().strip(), soup.find_all('span', 'recipe-directions__list--item')))\n",
    "    temp = list(filter(lambda x: x != '', temp))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directions_recipesplus(recipe):\n",
    "    return recipe['steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cut the orange with knife, to 4 quarters and bring out the pulps out the skins carefully.',\n",
       " 'Next make jelly drink as there on the packet is recommended.',\n",
       " 'Now add the pulps into the jelly and mix a bit. After wards pour the jelly mixture into a medium serving pot. Then put it into refrigerator.',\n",
       " 'Bring out the jelly after 2 hours.Cut a strip of water melon and a strip of cheese. Put the cheese on top, at the middle of jelly,s surface and put the water melon on top of the cheese.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions_recipesplus(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title\n",
    "This function should find **title** on the page. Takes single argument `soup` (BeautifulSoup), returns title as simple string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cookbooks get title\n",
    "def title_cookbooks(soup):\n",
    "    return soup.find_all('p', 'H2')[0].get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allrecipes get title\n",
    "def title_allrecipes(soup):\n",
    "    return list(map(lambda x: x.get_text().strip(), soup.find_all('h1','recipe-summary__h1')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients , directions and title testbed\n",
    "Test your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# use this code to test your functions on real life pages\n",
    "# note that not all urls from generator may be valid\n",
    "#\n",
    "### provide generator and proper functions\n",
    "test_generator = allrecipes_generator\n",
    "test_ingredients = ingredients_allrecipes\n",
    "test_directions = directions_allrecipes\n",
    "test_title = title_allrecipes\n",
    "\n",
    "for url, i in test_generator(0,4):\n",
    "    test_html_doc = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    test_soup = BeautifulSoup(test_html_doc, 'html.parser')\n",
    "    # assert title\n",
    "    title = test_title(test_soup)\n",
    "    assert type(title) is str\n",
    "    assert len(title) > 0\n",
    "    # assert ingredients\n",
    "    ing = test_ingredients(test_soup)\n",
    "    assert type(ing) is list\n",
    "    assert len(ing) > 0\n",
    "    for i in ing:\n",
    "        assert type(i) is str\n",
    "        assert len(i) > 0\n",
    "    direc = test_directions(test_soup)\n",
    "    assert type(direc) is list\n",
    "    assert len(direc) > 0\n",
    "    for i in direc:\n",
    "        assert type(i) is str\n",
    "        assert len(i) > 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix and suffix\n",
    "These functions are to generate prefix and suffix for filename (in case titles itself are not unique).  \n",
    "Prefix should end with '-' or other separator.  \n",
    "Suffix should start with '-' or other separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix\n",
    "str_id = lambda i: '0' * (8 - len(str(i))) + str(i) + \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target function\n",
    "Download, parse and save entire recipe - this function is executed in parallel. This is kind of universal - it is enough to provide url and set of proper parsing functions. Type void.  \n",
    "List of arguments:\n",
    "* **step** `int` - number of executed step\n",
    "* **url** `string` - url of page to be downloaded\n",
    "* **retrieve_ingredients** `function` - function retriving ingredients from `soup`\n",
    "* **retrieve_directions** `function` - function retrieving directions from `soup`\n",
    "* **retrieve_title** `function` - function retrieving title from `soup`\n",
    "* **path** *optional* `string` - custom path to save retrieved recipe\n",
    "* **filename_prefix** *optional* `string` - string value of filename prefix\n",
    "* **filename_suffix** *optional* `string` - string value of filename suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_recipe(step, url, retrieve_ingredients, retrieve_directions, retrieve_title, path=\"./\", filename_prefix=\"\", filename_suffix=\"\"):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        # get page\n",
    "        html_doc = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        # parse\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        # retrive information and save to dictionary\n",
    "        title = retrieve_title(soup)\n",
    "        recipe = dict()\n",
    "        recipe['title'] = title\n",
    "        recipe['ingredients'] = retrieve_ingredients(soup)\n",
    "        recipe['directions'] = retrieve_directions(soup)\n",
    "        recipe['link'] = url\n",
    "        # save to file\n",
    "        with open(path+'/'+filename_prefix+title.lower().replace(' ','_')+filename_suffix+'.json', 'w+') as f:\n",
    "            f.write(json.dumps(recipe))\n",
    "            \n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        \n",
    "        print(\"Step:\\t\", step, \"Time:\\t\", elapsed)\n",
    "    \n",
    "    except:\n",
    "        print(\"Unable to get recipe from:\\t\", url)\n",
    "        #for s in sys.exc_info():\n",
    "        #    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core function\n",
    "This function takes care of managing shild processes and running scrapping tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_core(begin, end, generator, \n",
    "                   ingredients, directions, title, \n",
    "                   target_func = save_recipe, path='./', prefix = None, suffix = None, \n",
    "                   sleep_time = 1, max_processes = 8):\n",
    "    #\n",
    "    # only downloads links from the list\n",
    "    #\n",
    "    \n",
    "    processes = []\n",
    "    for url, i in generator(begin, end):\n",
    "        \n",
    "        p = \"\"\n",
    "        s = \"\"\n",
    "        if prefix:\n",
    "            p = prefix(i)\n",
    "        if suffix:\n",
    "            s = suffix(i)\n",
    "\n",
    "        arguments = (i, url, ingredients, directions, title, path, p, s)\n",
    "            \n",
    "        inactive = []\n",
    "        # visit list of processes\n",
    "        for proc in processes:\n",
    "            # when process is no longer active, join it and add to list of inactive processes\n",
    "            if not proc.is_alive():\n",
    "                proc.join()\n",
    "                inactive.append(proc)\n",
    "        # remove inactive processes from processes list\n",
    "        while inactive:\n",
    "            processes.remove(inactive.pop())\n",
    "        # print(\"Number of active processes:\\t\", len(processes))\n",
    "    \n",
    "        # if number of active processes is acceptable, we can start new process\n",
    "        if len(processes) < max_processes: \n",
    "            p = mp.Process(target = target_func, args = arguments)\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        else:\n",
    "            print(\"List of processes is full\")\n",
    "    \n",
    "        # sleep, to avoid ddos attack or to fit in robots.txt rules\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # join remaining processes\n",
    "    while processes:\n",
    "        temp = processes.pop()\n",
    "        temp.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:\t 0 Time:\t 2.6241471767425537\n",
      "Step:\t 1 Time:\t 2.617486000061035\n",
      "Step:\t 2 Time:\t 2.373093843460083\n",
      "Step:\t 3 Time:\t 2.5032308101654053\n",
      "Step:\t 4 Time:\t 2.29815936088562\n"
     ]
    }
   ],
   "source": [
    "scrapping_core(0, 5, allrecipes_generator, ingredients_allrecipes, directions_allrecipes, title_allrecipes, prefix=str_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
