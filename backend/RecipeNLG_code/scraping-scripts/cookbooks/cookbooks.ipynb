{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom spider v0.1\n",
    "Second generation of custom spider, used to crawl cookbooks.com  \n",
    "Belive me or not, this shitty code managed to get almost 1 milion recipes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'cookbooks'\n",
    "url = 'http://www.cookbooks.com/Recipe-Details.aspx?id='\n",
    "last = 1086415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test page\n",
    "html_doc = urllib.request.urlopen('http://www.cookbooks.com/Recipe-Details.aspx?id='+str(1)).read().decode('utf-8')\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"H2\"><font size=\"4\"><i></i></font></span>,\n",
       " <span class=\"H2\">ingredients</span>,\n",
       " <span class=\"H2\">preparation</span>,\n",
       " <span class=\"H2\">Recipe Comments</span>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = soup.find_all('span', 'H2')\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Combine crumbs, nuts, 3 tablespoons granulated sugar and margarine.  Press onto bottom of 9-inch spring-form pan.  Bake at 325° for 10 minutes.  Combine cream cheese and brown sugar, mixing at medium speed with electric mixer until well blended.  Add eggs, one at a time, mixing well after each.  Blend in chocolate, milk, and vanilla extract; pour over crust.  Bake at 325° for 35 minutes. Combine sour cream and 2 tablespoons granulated sugar; carefully spread over cheesecake.  Bake at 425° for 10 minutes.  Loosen cake from rim of pan.  Cool before removing rim of pan.  Chill.  Garnish with mint candies.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[2].parent.p.get_text('\\n').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"H1\">1  c. vanilla wafer crumbs<br/>1/2  c. chopped pecans<br/>3  Tbsp. granulated sugar<br/>1/2  c. Parkay margarine<br/>2 (8 oz.)  pkg. Philadelphia cream cheese, softened<br/>1/2  c. packed brown sugar<br/>2  eggs<br/>16  oz. pkg. semi-sweet chocolate pieces<br/>2  Tbsp. milk<br/>1  tsp. vanilla extract<br/>2  c. sour cream<br/>2  Tbsp. granulated sugar<br/>  mint candies (garnish)<br/></p>,\n",
       " <p class=\"H1\">      Combine crumbs, nuts, 3 tablespoons granulated sugar and margarine.  Press onto bottom of 9-inch spring-form pan.  Bake at 325° for 10 minutes.  Combine cream cheese and brown sugar, mixing at medium speed with electric mixer until well blended.  Add eggs, one at a time, mixing well after each.  Blend in chocolate, milk, and vanilla extract; pour over crust.  Bake at 325° for 35 minutes. Combine sour cream and 2 tablespoons granulated sugar; carefully spread over cheesecake.  Bake at 425° for 10 minutes.  Loosen cake from rim of pan.  Cool before removing rim of pan.  Chill.  Garnish with mint candies.</p>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('p', 'H1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and parse ingredients\n",
    "def ingredients_cookbooks(soup):\n",
    "    ingredients_tag = soup.find_all('span', 'H2', string='ingredients')[0]\n",
    "    # print(ingredients_tag)\n",
    "    return ingredients_tag.parent.p.get_text('|').strip().split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and parse directions\n",
    "def directions_cookbooks(soup):\n",
    "    directions_tag = soup.find_all('span', 'H2', string='preparation')[0]\n",
    "    # print(directions_tag)\n",
    "    return directions_tag.parent.p.get_text('|').strip().split('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title\n",
    "def title_cookbooks(soup):\n",
    "    return soup.find_all('p', 'H2')[0].get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_cookbooks(soup):\n",
    "    ret = dict()\n",
    "    ret['title'] = title_cookbooks(soup)\n",
    "    ret['ingredients'] = ingredients_cookbooks(soup)\n",
    "    ret['directions'] = directions_cookbooks(soup)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_recipe(soup, retrive_recipe, retrive_title, path=\".\", filename_prefix='', filename_suffix=''):\n",
    "    title = retrive_title(soup)\n",
    "    body = json.dumps(retrive_recipe(soup))\n",
    "    # TODO check if such a file exists\n",
    "    with open(path+'/'+filename_prefix+title.lower().replace(' ','_')+filename_suffix+'.json', 'w+') as f:\n",
    "        f.write(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the proper function to be used in future and in framework\n",
    "def save_recipe(step, url, retrieve_ingredients, retrieve_directions, retrieve_title, path=\"./\", filename_prefix=\"\", filename_suffix=\"\"):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        # get page\n",
    "        html_doc = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "        # parse\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        # retrive information and save to dictionary\n",
    "        title = retrieve_title(soup)\n",
    "        recipe = dict()\n",
    "        recipe['title'] = title\n",
    "        recipe['ingredients'] = retrieve_ingredients(soup)\n",
    "        recipe['directions'] = retrieve_directions(soup)\n",
    "        # save to file\n",
    "        with open(path+'/'+filename_prefix+title.lower().replace(' ','_')+filename_suffix+'.json', 'w+') as f:\n",
    "            f.write(json.dumps(recipe))\n",
    "            \n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        \n",
    "        print(\"Step:\\t\", step, \"Time:\\t\", elapsed)\n",
    "    \n",
    "    except:\n",
    "        print(\"Unable to get recipe from:\\t\", url)\n",
    "        #for s in sys.exc_info():\n",
    "        #    print(s)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test\n",
    "for i in range(1,5):\n",
    "    # print(i)\n",
    "    try:\n",
    "        html_doc = urllib.request.urlopen('http://www.cookbooks.com/Recipe-Details.aspx?id='+str(i)).read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        print(recipe_cookbooks(soup))\n",
    "    except:\n",
    "        print(\"Unable to get recipe with id:\\t\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:\t 1 \tRecipes:\t 1 \tTime:\t 1.4868807792663574 \tMean time:\t 1.4868807792663574 \tMissing:\t 0.0\n",
      "Unable to get recipe with id:\t2\n",
      "Step:\t 3 \tRecipes:\t 2 \tTime:\t 1.280451774597168 \tMean time:\t 1.3836662769317627 \tMissing:\t 0.3333333333333333\n",
      "Unable to get recipe with id:\t4\n",
      "Step:\t 5 \tRecipes:\t 3 \tTime:\t 1.4655299186706543 \tMean time:\t 1.4109541575113933 \tMissing:\t 0.4\n",
      "Step:\t 6 \tRecipes:\t 4 \tTime:\t 1.321441888809204 \tMean time:\t 1.388576090335846 \tMissing:\t 0.3333333333333333\n",
      "Step:\t 7 \tRecipes:\t 5 \tTime:\t 1.3532140254974365 \tMean time:\t 1.3815036773681642 \tMissing:\t 0.2857142857142857\n",
      "Step:\t 8 \tRecipes:\t 6 \tTime:\t 1.954516887664795 \tMean time:\t 1.4770058790842693 \tMissing:\t 0.25\n",
      "Step:\t 9 \tRecipes:\t 7 \tTime:\t 1.2735438346862793 \tMean time:\t 1.4479398727416992 \tMissing:\t 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "recipes = []\n",
    "recipes_count = 0\n",
    "total = 0\n",
    "missed = 0\n",
    "counter = 0\n",
    "for i in range(1,10):\n",
    "    # print(i)\n",
    "    counter += 1\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        html_doc = urllib.request.urlopen('http://www.cookbooks.com/Recipe-Details.aspx?id='+str(i)).read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "        # recipes.append(recipe_cookbooks(soup))\n",
    "        \n",
    "        str_id = '0' * (8 - len(str(i))) + str(i) # since names/titles are not unique, id is added\n",
    "        \n",
    "        handle_recipe(soup, retrive_recipe=recipe_cookbooks, retrive_title=title_cookbooks, path='cookbooks/temp', filename_prefix=str_id+'-')\n",
    "        \n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        total += elapsed\n",
    "        \n",
    "        recipes_count += 1\n",
    "        \n",
    "        print(\"Step:\\t\", i, \"\\tRecipes:\\t\", recipes_count, \"\\tTime:\\t\", elapsed, \"\\tMean time:\\t\", total/recipes_count, '\\tMissing:\\t', missed/counter)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Finish and save\")\n",
    "        break\n",
    "        \n",
    "    except:\n",
    "        print(\"Unable to get recipe with id:\\t\" + str(i))\n",
    "        missed += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# random checkup - recipes list turned off\n",
    "recipes[random.randrange(0,len(recipes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for long tasks\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:\t 1 Time:\t 1.317464828491211\n",
      "Unable to get recipe from:\t http://www.cookbooks.com/Recipe-Details.aspx?id=2\n",
      "Step:\t 3 Time:\t 1.4252955913543701\n",
      "Unable to get recipe from:\t http://www.cookbooks.com/Recipe-Details.aspx?id=4\n",
      "Step:\t 5 Time:\t 1.318800687789917\n",
      "Step:\t 6 Time:\t 1.5576882362365723\n",
      "Step:\t 7 Time:\t 1.2769484519958496\n",
      "Step:\t 8 Time:\t 1.291440725326538\n",
      "Step:\t 9 Time:\t 1.4693281650543213\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# only downloads links from the list\n",
    "#\n",
    "\n",
    "sleep_time = 1\n",
    "max_processes = 8\n",
    "counter = 0\n",
    "\n",
    "seed = 'http://www.cookbooks.com/Recipe-Details.aspx?id='\n",
    "\n",
    "processes = []\n",
    "for i in range(1, 10):\n",
    "    # url selection or other custom shit here\n",
    "    str_id = '0' * (8 - len(str(i))) + str(i) # since names/titles are not unique, id is added\n",
    "    \n",
    "    arguments = (i, seed+str(i), ingredients_cookbooks, directions_cookbooks, title_cookbooks, 'cookbooks/temp', str_id+'-')\n",
    "    \n",
    "    target_func = save_recipe\n",
    "    \n",
    "    ### try not to edit code below this line\n",
    "    \n",
    "    counter += 1\n",
    "    inactive = []\n",
    "    # visit list of processes\n",
    "    for proc in processes:\n",
    "        # when process is no longer active, join it and add to list of inactive processes\n",
    "        if not proc.is_alive():\n",
    "            proc.join()\n",
    "            inactive.append(proc)\n",
    "    # remove inactive processes from processes list\n",
    "    while inactive:\n",
    "        processes.remove(inactive.pop())\n",
    "    # print(\"Number of active processes:\\t\", len(processes))\n",
    "    \n",
    "    # if number of active processes is acceptable, we can start new process\n",
    "    if len(processes) < max_processes: \n",
    "        p = mp.Process(target = target_func, args = arguments)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    else:\n",
    "        print(\"List of processes is full\")\n",
    "    \n",
    "    # sleep, to avoid ddos attack or to fit in robots.txt rules\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "# join remaining processes\n",
    "while processes:\n",
    "    temp = processes.pop()\n",
    "    temp.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find missing files\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = listdir('cookbooks/recipes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = 1086416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: int(x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619430"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(map(func, filenames))\n",
    "ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne = ids.pop(0)\n",
    "for i in range(1, last):\n",
    "    if len(ids) == 0 or i < ne:\n",
    "        missing.append(i)\n",
    "    elif i == ne:\n",
    "        ne = ids.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131741"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./cookbooks/missing.txt', 'w+') as f:\n",
    "    for m in missing:\n",
    "        f.write(str(m)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
